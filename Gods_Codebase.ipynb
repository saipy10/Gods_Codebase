{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12634900,"sourceType":"datasetVersion","datasetId":7983921}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":276.308338,"end_time":"2025-08-04T10:44:18.889938","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-04T10:39:42.5816","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 || Importing data and dependencies","metadata":{}},{"cell_type":"markdown","source":"## 1.1 || Importing dependencies","metadata":{}},{"cell_type":"code","source":"# <===========================================================================>\n# Data handling\n# <===========================================================================>\nimport pandas as pd\nimport numpy as np\nimport json\nfrom collections import Counter,defaultdict\nfrom itertools import combinations, chain\nimport random\nfrom ast import literal_eval\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport openpyxl\n\n# <===========================================================================>\n# Machine Learning\n# <===========================================================================>\nfrom gensim.models import Word2Vec, FastText\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import top_k_accuracy_score\nimport lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-07T10:53:29.264667Z","iopub.execute_input":"2025-08-07T10:53:29.26497Z","iopub.status.idle":"2025-08-07T10:54:00.776944Z","shell.execute_reply.started":"2025-08-07T10:53:29.264947Z","shell.execute_reply":"2025-08-07T10:54:00.776197Z"},"papermill":{"duration":48.432836,"end_time":"2025-08-04T10:40:35.987311","exception":false,"start_time":"2025-08-04T10:39:47.554475","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 || Loading Data","metadata":{}},{"cell_type":"code","source":"# <=== Load real datasets ===>\n# dataframe_name = pd.read_csv(\"Location/to/your/dataset\")\ncustomer_data = pd.read_csv(\"/kaggle/input/wwt-unravel-dataset/customer_data.csv\")\norder_data = pd.read_csv(\"/kaggle/input/wwt-unravel-dataset/order_data.csv\")\nstore_data = pd.read_csv(\"/kaggle/input/wwt-unravel-dataset/store_data.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/wwt-unravel-dataset/test_data_question.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-08-07T10:54:00.7787Z","iopub.execute_input":"2025-08-07T10:54:00.779546Z","iopub.status.idle":"2025-08-07T10:54:11.642963Z","shell.execute_reply.started":"2025-08-07T10:54:00.779518Z","shell.execute_reply":"2025-08-07T10:54:11.642019Z"},"papermill":{"duration":15.098895,"end_time":"2025-08-04T10:40:51.092114","exception":false,"start_time":"2025-08-04T10:40:35.993219","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 || Data Overview ","metadata":{}},{"cell_type":"markdown","source":"## 2.1 || Customer Data Analysis","metadata":{}},{"cell_type":"code","source":"# First few columns of customer data\ncustomer_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:11.644026Z","iopub.execute_input":"2025-08-07T10:54:11.644361Z","iopub.status.idle":"2025-08-07T10:54:11.657283Z","shell.execute_reply.started":"2025-08-07T10:54:11.64433Z","shell.execute_reply":"2025-08-07T10:54:11.656353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# customer data info and datatypes\ncustomer_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:11.659033Z","iopub.execute_input":"2025-08-07T10:54:11.659283Z","iopub.status.idle":"2025-08-07T10:54:11.711484Z","shell.execute_reply.started":"2025-08-07T10:54:11.659264Z","shell.execute_reply":"2025-08-07T10:54:11.710607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unique values in customer data\ncustomer_data[\"CUSTOMER_TYPE\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:11.712492Z","iopub.execute_input":"2025-08-07T10:54:11.7128Z","iopub.status.idle":"2025-08-07T10:54:11.762433Z","shell.execute_reply.started":"2025-08-07T10:54:11.712775Z","shell.execute_reply":"2025-08-07T10:54:11.76172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations and Inferences\n\n**Observations**\n- 406653 out of 563346 users are registered\n- 155132 out of 563346 are guest accounts\n- 348 are deleted accounts\n\n**Inferences**\n- 72.15% of users are registered on applications, which means people trying out services are satisfied with it. This is positive indication that current state of application can attract 72 out of 100 people trying out for a time.\n- 27.53% guest accounts refer to people that don't require these services that often. This means these are potential customers if application level ups.\n- 0.06% deleted accounts is positive indication of very low churn rate. This indicates that application is highly effective in fulfilling it's purpose. ","metadata":{}},{"cell_type":"markdown","source":"## 2.2 || Order Data Analysis","metadata":{}},{"cell_type":"code","source":"# First few columns of order data\norder_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:11.763315Z","iopub.execute_input":"2025-08-07T10:54:11.763539Z","iopub.status.idle":"2025-08-07T10:54:11.784109Z","shell.execute_reply.started":"2025-08-07T10:54:11.763517Z","shell.execute_reply":"2025-08-07T10:54:11.783123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# order data info and datatypes\norder_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:11.785067Z","iopub.execute_input":"2025-08-07T10:54:11.785407Z","iopub.status.idle":"2025-08-07T10:54:12.243509Z","shell.execute_reply.started":"2025-08-07T10:54:11.785377Z","shell.execute_reply":"2025-08-07T10:54:12.242735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `ORDERS` column contains the data in json format which needs to be handled properly","metadata":{}},{"cell_type":"code","source":"# <--- 1. Parse ORDERS column to extract item names --->\ndef extract_items(order_json_str):\n    try:\n        order_dict = json.loads(order_json_str)\n        items = []\n        for order in order_dict.get(\"orders\", []):\n            for item in order.get(\"item_details\", []):\n                if item.get(\"item_price\", 0) > 0:\n                    items.append(item.get(\"item_name\"))\n        return items\n    except:\n        return []\norder_data[\"item_list\"] = order_data[\"ORDERS\"].apply(extract_items)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:12.244348Z","iopub.execute_input":"2025-08-07T10:54:12.244605Z","iopub.status.idle":"2025-08-07T10:54:23.821028Z","shell.execute_reply.started":"2025-08-07T10:54:12.244563Z","shell.execute_reply":"2025-08-07T10:54:23.819939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"<==================================================================>\")\ndisplay(order_data[\"ORDER_CHANNEL_NAME\"].value_counts())\nprint(\"<==================================================================>\")\nprint(\"<==================================================================>\")\ndisplay(order_data[\"ORDER_SUBCHANNEL_NAME\"].value_counts())\nprint(\"<==================================================================>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:23.822972Z","iopub.execute_input":"2025-08-07T10:54:23.823251Z","iopub.status.idle":"2025-08-07T10:54:24.052468Z","shell.execute_reply.started":"2025-08-07T10:54:23.823227Z","shell.execute_reply":"2025-08-07T10:54:24.05177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get value counts\nsubchannel_counts = order_data[\"ORDER_SUBCHANNEL_NAME\"].value_counts()\n\n# Plot pie chart\nplt.figure(figsize=(4,4))\nplt.pie(\n    subchannel_counts,\n    labels=subchannel_counts.index,\n    autopct='%1.2f%%',\n    startangle=140,\n    wedgeprops={'edgecolor': 'black'}\n)\nplt.title('Order Distribution by Subchannel')\nplt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.055527Z","iopub.execute_input":"2025-08-07T10:54:24.055836Z","iopub.status.idle":"2025-08-07T10:54:24.269762Z","shell.execute_reply.started":"2025-08-07T10:54:24.055815Z","shell.execute_reply":"2025-08-07T10:54:24.268849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(order_data[\"ORDER_OCCASION_NAME\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.270794Z","iopub.execute_input":"2025-08-07T10:54:24.271115Z","iopub.status.idle":"2025-08-07T10:54:24.383288Z","shell.execute_reply.started":"2025-08-07T10:54:24.271094Z","shell.execute_reply":"2025-08-07T10:54:24.382455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get value counts\noccasion_counts = order_data[\"ORDER_OCCASION_NAME\"].value_counts()\n\n# Plot pie chart\nplt.figure(figsize=(4,4))\nplt.pie(\n    occasion_counts,\n    labels=occasion_counts.index,\n    autopct='%1.2f%%',\n    startangle=140,\n    wedgeprops={'edgecolor': 'black'}\n)\nplt.title('Order Distribution by Occasion')\nplt.axis('equal')  # Equal aspect ratio ensures pie is a circle\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.384536Z","iopub.execute_input":"2025-08-07T10:54:24.384857Z","iopub.status.idle":"2025-08-07T10:54:24.59267Z","shell.execute_reply.started":"2025-08-07T10:54:24.384834Z","shell.execute_reply":"2025-08-07T10:54:24.591794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations and Inferences\n\n\n**Observations:**\n\n- Only **one channel** is recorded: `Digital`, with **1,414,410 orders**.\n- This implies **100%** of the orders were placed through digital platforms.\n- Majority of the orders (**1,411,089 orders**, \\~99.77%) came from the `WWT` subchannel.\n- A small fraction (**3,321 orders**, \\~0.23%) came from the `Grub Hub Marketplace`.\n- ToGo: 1,214,921 orders (~85.9%)\n- Delivery: 199,489 orders (~14.1%)\n\n\n**Inferences:**\n\n- The business currently operates exclusively through digital ordering, with no evidence of in-store, phone, or third-party offline orders in the dataset.\n- `WWT` is the **primary digital subchannel**, likely the business's **own website or mobile app**.\n- This suggests a **strong direct customer base** that prefers ordering through the official platform.\n- Thus taking these two features into account for now would be of no difference for our model.\n- A majority of customers (~86%) prefer placing ToGo (pickup) orders.\n- Only 14% of orders are delivery-based, which could indicate.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 || Store Data Analysis","metadata":{}},{"cell_type":"code","source":"# First few columns of store data\nstore_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.593565Z","iopub.execute_input":"2025-08-07T10:54:24.593925Z","iopub.status.idle":"2025-08-07T10:54:24.603377Z","shell.execute_reply.started":"2025-08-07T10:54:24.593895Z","shell.execute_reply":"2025-08-07T10:54:24.602382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# store data info and datatypes\nstore_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.604432Z","iopub.execute_input":"2025-08-07T10:54:24.6049Z","iopub.status.idle":"2025-08-07T10:54:24.623872Z","shell.execute_reply.started":"2025-08-07T10:54:24.604868Z","shell.execute_reply":"2025-08-07T10:54:24.622897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations and Inferences\n\n**Observations**\n\n- 38 `STORE_NUMBER`s are present in store_data\n- 25 `CITY` names are present\n- 24 `STATE` and 36 `POSTAL_CODE`\n\n**Inferences**\n\n-  `STORE_NUMBER` is only reliable feature because there is no null value present.\n-  Complete `STORE_NUMBER` can aptly present regional preferences instead of incomplete columns or data.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 || Test Data Analysis","metadata":{}},{"cell_type":"code","source":"# First few columns of test data\ntest_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.624785Z","iopub.execute_input":"2025-08-07T10:54:24.62503Z","iopub.status.idle":"2025-08-07T10:54:24.646519Z","shell.execute_reply.started":"2025-08-07T10:54:24.625011Z","shell.execute_reply":"2025-08-07T10:54:24.645722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test data info\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.647406Z","iopub.execute_input":"2025-08-07T10:54:24.647701Z","iopub.status.idle":"2025-08-07T10:54:24.666072Z","shell.execute_reply.started":"2025-08-07T10:54:24.64768Z","shell.execute_reply":"2025-08-07T10:54:24.665101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CUSTOMER_ID and ORDER_ID relevance as a feature\n# Unique sets\norder_customer_ids = set(order_data[\"CUSTOMER_ID\"].unique())\norder_order_ids = set(order_data[\"ORDER_ID\"].unique())\n\ntest_customer_ids = set(test_data[\"CUSTOMER_ID\"].unique())\ntest_order_ids = set(test_data[\"ORDER_ID\"].unique())\n\n# Differences\nmissing_customer_ids = test_customer_ids - order_customer_ids\nmissing_order_ids = test_order_ids - order_order_ids\n\n# Results\nprint(f\"CUSTOMER_IDs in test_df but not in order_data: {len(missing_customer_ids)}\")\nprint(f\"ORDER_IDs in test_df but not in order_data: {len(missing_order_ids)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:24.667084Z","iopub.execute_input":"2025-08-07T10:54:24.667389Z","iopub.status.idle":"2025-08-07T10:54:25.904109Z","shell.execute_reply.started":"2025-08-07T10:54:24.667366Z","shell.execute_reply":"2025-08-07T10:54:25.903152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(test_data[\"ORDER_CHANNEL_NAME\"].value_counts())\nprint(\"<======================================>\")\ndisplay(test_data[\"ORDER_SUBCHANNEL_NAME\"].value_counts())\nprint(\"<======================================>\")\ntest_data[\"ORDER_OCCASION_NAME\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:25.905012Z","iopub.execute_input":"2025-08-07T10:54:25.905259Z","iopub.status.idle":"2025-08-07T10:54:25.919337Z","shell.execute_reply.started":"2025-08-07T10:54:25.905239Z","shell.execute_reply":"2025-08-07T10:54:25.918607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observations and Inferences\n\n**Observations**\n- `ORDER_CHANNEL_NAME` and `ORDER_SUBCHANNEL_NAME` contains only single unique value.\n- `CUSTOMER_ID` contains 500 values that are not present in order dataset.\n- `ORDER_ID` contains 1000 values that are absent in order dataset.\n- `ORDER_OCCASION_NAME` contains 853 *ToGo* values and 147 *Delivery* values.\n\n**Inferences**\n- Since single value is inappropriate to guide the decisions, hence `ORDER_CHANNEL_NAME` and `ORDER_SUBCHANNEL_NAME` shouldn't be used as of the current state of application or of the dataset.\n- `CUSTOMER_ID` should be seen more as discrete column rather than continuous one because continuous nature  of ids doesn't signify any signal for recommendation.\n- `ORDER_ID` should be treated in similar concepts as `CUSTOMER_ID`.\n- `ORDER_OCCASION_NAME` is proper feature that guides for making predictions.","metadata":{}},{"cell_type":"markdown","source":"# 3 || Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 3.1 || Creating train dataframe","metadata":{}},{"cell_type":"code","source":"order_data[\"item_list\"].head()","metadata":{"papermill":{"duration":0.016567,"end_time":"2025-08-04T10:41:02.731405","exception":false,"start_time":"2025-08-04T10:41:02.714838","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:25.920186Z","iopub.execute_input":"2025-08-07T10:54:25.920436Z","iopub.status.idle":"2025-08-07T10:54:25.936442Z","shell.execute_reply.started":"2025-08-07T10:54:25.920416Z","shell.execute_reply":"2025-08-07T10:54:25.935535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"order_data[\"item_count\"] = order_data[\"item_list\"].apply(len)","metadata":{"papermill":{"duration":11.419597,"end_time":"2025-08-04T10:41:14.156721","exception":false,"start_time":"2025-08-04T10:41:02.737124","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:25.937296Z","iopub.execute_input":"2025-08-07T10:54:25.937531Z","iopub.status.idle":"2025-08-07T10:54:26.492147Z","shell.execute_reply.started":"2025-08-07T10:54:25.937512Z","shell.execute_reply":"2025-08-07T10:54:26.491148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure CUSTOMER_TYPE is available in order_data\norder_data = order_data.merge(\n    customer_data[[\"CUSTOMER_ID\", \"CUSTOMER_TYPE\"]],\n    on=\"CUSTOMER_ID\",\n    how=\"left\"\n)\n\n# Then proceed to filter and build training data\nfiltered_orders = order_data[order_data[\"item_count\"] >= 4].copy()","metadata":{"papermill":{"duration":1.025127,"end_time":"2025-08-04T10:41:15.187186","exception":false,"start_time":"2025-08-04T10:41:14.162059","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:26.493349Z","iopub.execute_input":"2025-08-07T10:54:26.493925Z","iopub.status.idle":"2025-08-07T10:54:27.560331Z","shell.execute_reply.started":"2025-08-07T10:54:26.493897Z","shell.execute_reply":"2025-08-07T10:54:27.559637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_rows = []\n\nfor _, row in tqdm(order_data.iterrows(), total=len(order_data)):\n    item_seq = row[\"item_list\"]\n    \n    # Only process orders with exactly 4 or more items\n    if len(item_seq) < 4:\n        continue\n\n    unique_items = list(set(item_seq))  # remove duplicates\n    if len(unique_items) != 4:\n        continue\n\n    for i in range(4):  # 4 combinations\n        target = unique_items[i]\n        inputs = sorted([unique_items[j] for j in range(4) if j != i])  # pick other 3, sorted lex\n        train_rows.append({\n            \"CUSTOMER_ID\": row[\"CUSTOMER_ID\"],\n            \"STORE_NUMBER\": row[\"STORE_NUMBER\"],\n            \"ORDER_OCCASION_NAME\": row[\"ORDER_OCCASION_NAME\"],\n            \"CUSTOMER_TYPE\": row[\"CUSTOMER_TYPE\"],\n            \"input_item1\": inputs[0],\n            \"input_item2\": inputs[1],\n            \"input_item3\": inputs[2],\n            \"target_item\": target\n        })","metadata":{"papermill":{"duration":66.464586,"end_time":"2025-08-04T10:42:21.657139","exception":false,"start_time":"2025-08-04T10:41:15.192553","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:54:27.561258Z","iopub.execute_input":"2025-08-07T10:54:27.561513Z","iopub.status.idle":"2025-08-07T10:55:32.365224Z","shell.execute_reply.started":"2025-08-07T10:54:27.561492Z","shell.execute_reply":"2025-08-07T10:55:32.364485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.DataFrame(train_rows)\ntrain_df.info()","metadata":{"papermill":{"duration":0.590471,"end_time":"2025-08-04T10:42:22.285787","exception":false,"start_time":"2025-08-04T10:42:21.695316","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:32.366192Z","iopub.execute_input":"2025-08-07T10:55:32.366458Z","iopub.status.idle":"2025-08-07T10:55:33.179442Z","shell.execute_reply.started":"2025-08-07T10:55:32.366439Z","shell.execute_reply":"2025-08-07T10:55:33.178515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 || Purchase Quantity Pattern Analysis","metadata":{}},{"cell_type":"code","source":"order_data[\"item_count\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:33.180445Z","iopub.execute_input":"2025-08-07T10:55:33.180791Z","iopub.status.idle":"2025-08-07T10:55:33.196701Z","shell.execute_reply.started":"2025-08-07T10:55:33.180759Z","shell.execute_reply":"2025-08-07T10:55:33.195935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute item count frequencies\nitem_count_freq = order_data[\"item_count\"].value_counts().sort_index()\n\n# Plot bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(item_count_freq.index, item_count_freq.values, edgecolor='black')\n\n# Add labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:,}', \n             ha='center', va='bottom', fontsize=9)\n\n# Aesthetics\nplt.title(\"Distribution of Item Count per Order\")\nplt.xlabel(\"Number of Items in an Order\")\nplt.ylabel(\"Number of Orders\")\nplt.xticks(item_count_freq.index)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:33.197652Z","iopub.execute_input":"2025-08-07T10:55:33.198001Z","iopub.status.idle":"2025-08-07T10:55:33.521099Z","shell.execute_reply.started":"2025-08-07T10:55:33.197972Z","shell.execute_reply":"2025-08-07T10:55:33.520098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train Data Justification\n- Basket size correlates positively with add-on probability — 3-item shoppers are statistically more inclined to keep shopping.\n- Association rule mining shows higher confidence/lift when recommending a 4th item from 3-item sets versus 1- or 2-item sets.\n- Consumers view 3 items as \"almost complete\", often adding a 4th for convenience, variety, or value.\n- 3-item baskets enable stronger collaborative filtering, enhancing next-item recommendation accuracy.\n- Combinatorial diversity is richer at size 3, yielding better predictive power for the 4th item via modeling.\n- Many retail promotions and bundles target 4+ items, encouraging add-ons from 3-item carts.\n- Market basket studies reveal that mid-sized baskets (3–5 items) are most likely to expand further.\n- Shoppers with 3 items are more engaged, signaling higher intent and openness to suggestions.\n- Cross-selling opportunities increase when 3 complementary or diverse items are already present.\n- Recommendation systems trained on 3-item inputs consistently show higher precision and recall for predicting the next item","metadata":{}},{"cell_type":"markdown","source":"## 3.3 || Churn Risk: Customers with Only 1 Order","metadata":{}},{"cell_type":"code","source":"# Calculate order counts\ncust_order_counts = order_data[\"CUSTOMER_ID\"].value_counts()\nchurners = (cust_order_counts == 1).sum()\nnon_churners = (cust_order_counts > 1).sum()\n\n# Data for plotting\nlabels = [\"1 Order (Churn Risk)\", \"2+ Orders\"]\ncounts = [churners, non_churners]\ncolors = [\"red\", \"green\"]\ntotal = churners + non_churners\n\n# Plot\nplt.figure(figsize=(5, 5))\nbars = plt.bar(labels, counts, color=colors)\nplt.title(\"Churn Risk: Customers with Only 1 Order\")\nplt.ylabel(\"Number of Customers\")\n\n# Annotate bars with count and percentage\nfor bar, count in zip(bars, counts):\n    percent = (count / total) * 100\n    label = f\"({percent:.1f}%)\"\n    plt.text(bar.get_x() + bar.get_width()/2, count, label,\n             ha='center', va='bottom', fontsize=12)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:33.522043Z","iopub.execute_input":"2025-08-07T10:55:33.522305Z","iopub.status.idle":"2025-08-07T10:55:33.92829Z","shell.execute_reply.started":"2025-08-07T10:55:33.522285Z","shell.execute_reply":"2025-08-07T10:55:33.927375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inferences\n\n- **No Repeat Orders**: Customers with only 1 order haven't shown continued interest, which is a key indicator of churn in most industries.\n\n- **Stuck in Trial Phase**: These customers likely haven’t moved beyond trying the product/service, meaning they haven't been converted into loyal users.\n\n- **Low Lifetime Value (LTV)**: One-time buyers contribute very little revenue over time, making them less valuable unless re-engaged.\n\n- **Possible Dissatisfaction**: They might have faced issues with product quality, service, delivery, or pricing — leading them to not return.\n\n- **Unpredictable Behavior**: With just one transaction, there's insufficient data to accurately personalize recommendations or marketing.\n\n- **High Risk of Switching**: These users may have been testing your brand and could easily shift to a competitor if not nurtured.\n\n- **Opportunity for Targeted Campaigns**: Identifying them allows for win-back strategies (e.g., discounts, reminders) to increase retention.\n\n- **Supported by Churn Analytics**: Historical customer data often reveals that a significant portion of churned users had only one purchase.","metadata":{}},{"cell_type":"markdown","source":"## 3.4 || Order Size Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\n# Plot histogram and capture returned values\ncounts, bins, patches = plt.hist(\n    order_data[\"item_count\"], \n    bins=range(1, order_data[\"item_count\"].max() + 2), \n    edgecolor='black'\n)\n\n# Add numbers on top of each bar\nfor count, bin_edge, patch in zip(counts, bins, patches):\n    if count > 0:  # Avoid labeling empty bars\n        plt.text(\n            patch.get_x() + patch.get_width()/2,  # X position\n            count,  # Y position\n            f\"{int(count):,}\",  # Label text (with comma separator)\n            ha='center', va='bottom', fontsize=10\n        )\n\nplt.title(\"Distribution of Item Count per Order\")\nplt.xlabel(\"Number of Items\")\nplt.ylabel(\"Number of Orders\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:33.92927Z","iopub.execute_input":"2025-08-07T10:55:33.929516Z","iopub.status.idle":"2025-08-07T10:55:34.225926Z","shell.execute_reply.started":"2025-08-07T10:55:33.92949Z","shell.execute_reply":"2025-08-07T10:55:34.225025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inferences\n- Most orders contain only 1–3 items, indicating typical small-size purchases.\n\n- There's a long tail of larger orders, suggesting occasional bulk or group orders.\n\n- High frequency of low item counts reflects individual or minimal meal behavior.\n\n- Low average basket size reveals potential for upselling or combo offers.\n\n- Distribution insights aid in optimizing inventory and kitchen operations.\n\n- Very large orders may indicate outliers or special use cases worth analyzing.\n\n- Item count segments can guide personalized marketing and promotions.","metadata":{}},{"cell_type":"markdown","source":"## 3.5 || Top-Selling Combinations","metadata":{}},{"cell_type":"code","source":"combo_counter = Counter()\nfor items in order_data[\"item_list\"]:\n    unique_items = list(set(items))\n    if len(unique_items) >= 3:\n        for combo in combinations(sorted(unique_items), 3):\n            combo_counter[combo] += 1\n        if len(unique_items) >= 4:\n            for combo in combinations(sorted(unique_items), 4):\n                combo_counter[combo] += 1\n\ntop_combos = combo_counter.most_common(10)\nlabels = ['\\n'.join(combo) for combo, _ in top_combos]\nvalues = [v for _, v in top_combos]\n\nplt.figure(figsize=(12, 12))\nplt.barh(labels, values)\nplt.title(\"Top-Selling Item Combinations (3 or 4 items)\")\nplt.xlabel(\"Frequency\")\nplt.gca().invert_yaxis()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:34.230631Z","iopub.execute_input":"2025-08-07T10:55:34.230924Z","iopub.status.idle":"2025-08-07T10:55:37.81883Z","shell.execute_reply.started":"2025-08-07T10:55:34.230902Z","shell.execute_reply":"2025-08-07T10:55:37.817848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inferences\n\n- **Popular Bundles Identified**: The most frequent item combinations reveal popular bundle patterns among customers.\n\n- **Strong Item Affinity**: Repeated combos suggest strong product affinity and natural pairing behavior.\n\n- **Bundle Creation Opportunities**: These combos can be used to create preset meal bundles or value combos.\n\n- **Influenced by Menu Design**: High-frequency combinations may reflect default user preferences or menu structure influence.\n\n- **Potential to Boost AOV**: Promoting these combos can increase AOV (Average Order Value) through upselling.\n\n- **Menu Structuring Insight**: These patterns can guide effective menu design and targeted recommendations.\n\n- **High-Value Customer Signals**: 4-item combos appearing frequently indicate a segment of high-value or bulk-order customers.\n\n- **Seasonal Shifts Possible**: Combo trends can shift over time and should be monitored seasonally.\n\n- **Data-Driven Promotions**: Insights from combos help optimize promotional and pricing strategies.\n\n- **Supports Inventory Planning**: Understanding co-purchase behavior supports better inventory and kitchen planning.","metadata":{}},{"cell_type":"markdown","source":"## 3.6 || Most Frequently Purchased Items","metadata":{}},{"cell_type":"code","source":"# Count item frequency\nitem_counter = Counter()\nfor items in order_data[\"item_list\"]:\n    item_counter.update(items)\n\n# Get top 10 items\ntop_items = item_counter.most_common(10)\nlabels, values = zip(*top_items)\n\n# Plot\nplt.figure(figsize=(10, 6))\nbars = plt.bar(labels, values, color='orange')\nplt.title(\"Most Frequently Purchased Items\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Count\")\n\n# Show counts on top of bars\nfor bar, count in zip(bars, values):\n    plt.text(\n        bar.get_x() + bar.get_width()/2,\n        count,\n        f\"{count:,}\",     # Add comma formatting\n        ha='center', va='bottom', fontsize=11\n    )\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:37.819887Z","iopub.execute_input":"2025-08-07T10:55:37.820191Z","iopub.status.idle":"2025-08-07T10:55:39.148477Z","shell.execute_reply.started":"2025-08-07T10:55:37.820162Z","shell.execute_reply":"2025-08-07T10:55:39.147614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inferences\n\n- **Core Menu Drivers**: The top 10 items represent the backbone of customer demand and drive the majority of sales volume.\n\n- **High Popularity Concentration**: A few items dominate purchases, suggesting a skewed distribution where certain products vastly outperform others.\n\n- **Menu Optimization Potential**: These high-performing items can be given more visibility in menus, promotions, and bundles.\n\n- **Inventory Prioritization**: Consistently high-demand items should be prioritized in inventory planning and stock management.\n\n- **Customer Favorites Insight**: These items likely reflect core customer preferences and should be preserved across menu updates.\n\n- **Ideal for Promotions**: Frequently purchased items are ideal for limited-time promotions to attract repeat buyers.\n\n- **Cross-Sell Anchors**: Top items can serve as anchors in combo deals or personalized recommendation engines.\n\n- **Operational Planning Support**: Knowledge of fast-moving items helps optimize kitchen workflows and reduce preparation time.\n\n- **Price Sensitivity Monitoring**: Any price changes in these items should be monitored closely due to their large impact on overall sales.\n\n- **Feedback Focus Areas**: These items are also priority candidates for collecting and analyzing customer feedback, as they impact a large user base.","metadata":{}},{"cell_type":"markdown","source":"## 3.7 || Item Co-occurrence Matrix (Top 50 Items)","metadata":{}},{"cell_type":"code","source":"top_items = [item for item, _ in item_counter.most_common(50)]\nitem_index = {item: i for i, item in enumerate(top_items)}\nco_matrix = np.zeros((50, 50), dtype=int)\n\nfor items in order_data[\"item_list\"]:\n    present_items = [item for item in set(items) if item in item_index]\n    for i in range(len(present_items)):\n        for j in range(len(present_items)):\n            if i != j:\n                co_matrix[item_index[present_items[i]], item_index[present_items[j]]] += 1\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(co_matrix, xticklabels=top_items, yticklabels=top_items, cmap='Blues')\nplt.title(\"Item Co-occurrence Matrix (Top 50 Items)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:39.149719Z","iopub.execute_input":"2025-08-07T10:55:39.150008Z","iopub.status.idle":"2025-08-07T10:55:45.103124Z","shell.execute_reply.started":"2025-08-07T10:55:39.149982Z","shell.execute_reply":"2025-08-07T10:55:45.102089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inferences\n\n- **High Co-Occurrence Indicates Pairing Behavior**: Items that frequently appear together (darker cells) suggest strong co-purchase tendencies.\n\n- **Popular Combos Reinforced**: Frequently co-occurring pairs often reflect top-selling combos seen in earlier analyses.\n\n- **Complementary Product Insights**: Co-occurrence patterns can identify which items naturally complement each other (e.g., sides + mains, drinks + combos).\n\n- **Combo and Bundle Design**: Pairs or clusters of items with strong co-occurrence can be turned into fixed-value meal deals or combo offers.\n\n- **Recommendation System Input**: This matrix is a powerful base for item-item collaborative filtering in recommendation engines.\n\n- **Menu Placement Strategy**: Items often bought together can be placed adjacent in digital menus or app interfaces to increase order size.\n\n- **Inventory Dependency Mapping**: Strong co-occurrence implies shared demand cycles, useful for synchronized inventory planning.\n\n- **Customer Behavior Clustering**: Rows/columns with similar co-occurrence profiles may reflect similar customer segments or ordering behaviors.\n\n- **Sparse Areas Reveal Gaps**: Low co-occurrence zones (lighter areas) may suggest untapped bundling opportunities or rarely paired items.\n\n- **Temporal or Regional Extension**: Similar matrices can be built per store or season to detect local or time-based co-purchase trends.","metadata":{}},{"cell_type":"markdown","source":"# 4 || Model Training\n","metadata":{}},{"cell_type":"markdown","source":"## Approach","metadata":{}},{"cell_type":"markdown","source":"### 🧠 Overview\n\nThis system recommends the **4th item** in an order given 3 input items, based on patterns learned from historical customer behavior. It uses a **multi-level dictionary-based lookup strategy** to balance **accuracy and performance**, allowing quick recommendations even at scale.","metadata":{}},{"cell_type":"markdown","source":"## 4.1 || Data Processing","metadata":{}},{"cell_type":"markdown","source":"### **1. Item Encoding for Speed**\n\n* Converts all unique item names to integer IDs using `item2id`.\n* This improves speed and memory usage, making operations vectorizable and hash-map friendly.\n\n>  **Why**: Encoding avoids repeated string comparisons and speeds up lookups.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\n# ========= Step 1: Encode items =========\n\n# Get all unique items from sequences\nsequences = order_data[\"item_list\"].dropna().tolist()\nunique_items = set(chain.from_iterable(sequences))\n\n# Build item <-> id maps\nitem2id = {item: idx for idx, item in enumerate(sorted(unique_items))}\nid2item = {idx: item for item, idx in item2id.items()}\n\n# ========= Step 2: Encode train_df =========\n\ntrain_encoded = train_df.copy()\n\n# Encode and drop unknowns\ndef encode_or_none(item):\n    return item2id.get(item, None)\n\ntrain_encoded['input_item1'] = train_encoded['input_item1'].map(encode_or_none)\ntrain_encoded['input_item2'] = train_encoded['input_item2'].map(encode_or_none)\ntrain_encoded['input_item3'] = train_encoded['input_item3'].map(encode_or_none)\ntrain_encoded['target_item'] = train_encoded['target_item'].map(encode_or_none)\n\ntrain_encoded.dropna(inplace=True)\ntrain_encoded = train_encoded.astype({'input_item1': int, 'input_item2': int, 'input_item3': int, 'target_item': int})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:45.104315Z","iopub.execute_input":"2025-08-07T10:55:45.104636Z","iopub.status.idle":"2025-08-07T10:55:46.208504Z","shell.execute_reply.started":"2025-08-07T10:55:45.104608Z","shell.execute_reply":"2025-08-07T10:55:46.207791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 || Building Lookup","metadata":{}},{"cell_type":"markdown","source":"### **2. Input Triplet Sorting**\n\n* Each group of 3 items is sorted before being used as a key.\n* This ensures that permutations like (`itemA`, `itemB`, `itemC`) and (`itemC`, `itemB`, `itemA`) are treated the same.\n\n> ✅ **Why**: Consistent key generation ensures more matches and avoids duplicates.","metadata":{}},{"cell_type":"code","source":"# Step 2: Build lookup dictionaries\ndef build_lookup(df):\n    full_match = defaultdict(Counter)\n    partial_match = defaultdict(Counter)\n    item_only = defaultdict(Counter)\n\n    for _, row in df.iterrows():\n        key5 = (row['STORE_NUMBER'], row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key4 = (row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key3 = tuple(sorted([row['input_item1'], row['input_item2'], row['input_item3']]))\n\n        full_match[key5][row['target_item']] += 1\n        partial_match[key4][row['target_item']] += 1\n        item_only[key3][row['target_item']] += 1\n\n    return full_match, partial_match, item_only\n\nfull_match_dict, partial_match_dict, item_only_dict = build_lookup(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:55:46.209425Z","iopub.execute_input":"2025-08-07T10:55:46.209751Z","iopub.status.idle":"2025-08-07T10:56:14.794217Z","shell.execute_reply.started":"2025-08-07T10:55:46.209723Z","shell.execute_reply":"2025-08-07T10:56:14.79326Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 || Recommendation System (Lookup Based Algorithm)","metadata":{}},{"cell_type":"markdown","source":"### **3. Multi-Level Lookup Dictionaries**\n\n#### `full_match_dict`\n\n* Key: (`STORE_NUMBER`, `CUSTOMER_TYPE`, `input_item1`, `input_item2`, `input_item3`)\n* High precision, low recall. Used when all context is available.\n\n#### `partial_match_dict`\n\n* Key: (`CUSTOMER_TYPE`, `input_item1`, `input_item2`, `input_item3`)\n* Drops store info, offering a broader match with slightly reduced accuracy.\n\n#### `item_only_dict`\n\n* Key: Only (`input_item1`, `input_item2`, `input_item3`)\n* Widest scope. Used as fallback when no customer/store info match is found.\n\n> ✅ **Why**: These levels act as fallback layers. The system can still make a prediction even with partial information.\n","metadata":{}},{"cell_type":"code","source":"# Step 3: Recommendation function with fallback logic and deduplication\ndef get_recommendations(store_number, customer_type, item1, item2, item3):\n    items_sorted = sorted([item1, item2, item3])\n    key5 = (store_number, customer_type, *items_sorted)\n    key4 = (customer_type, *items_sorted)\n    key3 = tuple(items_sorted)\n\n    collected = []\n    seen = set()\n\n    # Try full match\n    if key5 in full_match_dict:\n        for item, _ in full_match_dict[key5].most_common():\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                return collected\n\n    # If not enough, use partial match\n    if key4 in partial_match_dict:\n        for item, _ in partial_match_dict[key4].most_common():\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                return collected\n\n    # If still not enough, use item-only match\n    if key3 in item_only_dict:\n        for item, _ in item_only_dict[key3].most_common():\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                return collected\n\n    # Fill remaining with NOT_FOUND\n    while len(collected) < 3:\n        collected.append(\"NOT_FOUND\")\n\n    return collected","metadata":{"papermill":{"duration":38.855182,"end_time":"2025-08-04T10:43:01.176949","exception":false,"start_time":"2025-08-04T10:42:22.321767","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:56:14.795267Z","iopub.execute_input":"2025-08-07T10:56:14.795562Z","iopub.status.idle":"2025-08-07T10:56:14.803361Z","shell.execute_reply.started":"2025-08-07T10:56:14.795534Z","shell.execute_reply":"2025-08-07T10:56:14.802558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. Recommendation Function**\n\n* Checks each level in order of specificity: full → partial → item-only.\n* Collects the **top 3 most common target items** that were historically seen with the same input triplet.\n* Pads with `-1` if fewer than 3 recommendations are found.\n\n> ✅ **Why**: Balances precision with coverage. Offers recommendations even with missing or incomplete metadata.\n\n---\n\n### **5. Evaluation with Recall\\@3**\n\n* Measures how often the correct target item appears in the top 3 predictions.\n* Used to track the effectiveness of the system on historical data.\n\n> ✅ **Why**: Recall\\@3 is ideal for real-world use cases where showing 3 good options is often enough (e.g., food apps, e-commerce).\n\n","metadata":{}},{"cell_type":"code","source":"# Step 4: Evaluate Recall@3 on sample train\nsample_eval = train_df.sample(n=5000, random_state=42)\nhits = 0\n\nfor i, (_, row) in enumerate(tqdm(sample_eval.iterrows(), total=len(sample_eval)), start=1):\n    if i % 500 == 0:\n        print(f\"📈 Recall@3 after {i} records: {hits / i:.4f}\")\n\n    store = row['STORE_NUMBER']\n    cust_type = row['CUSTOMER_TYPE']\n    item1, item2, item3 = sorted([row['input_item1'], row['input_item2'], row['input_item3']])\n    target = row['target_item']\n\n    preds = get_recommendations(store, cust_type, item1, item2, item3)\n\n    if target in preds:\n        hits += 1\n\n    key5 = (store, cust_type, item1, item2, item3)\n    key4 = (cust_type, item1, item2, item3)\n    key3 = (item1, item2, item3)\n\n    # <=============================DEBUG Lines======================================>\n    # <==================Uncomment for Better Understanding==========================>\n    # print(f\"\\n🔍 Record {idx}\")\n    # print(f\"🧾 Input: STORE={store}, TYPE={cust_type}, ITEMS=[{item1}, {item2}, {item3}]\")\n    # print(f\"🎯 Actual Target: {target}\")\n    # print(f\"🤖 Predicted: {preds}\")\n\n    # if key5 in full_match_dict:\n    #     print(\"✅ Match Type: FULL (5 fields)\")\n    #     print(\"📂 FULL targets:\", dict(full_match_dict[key5]))\n    # else:\n    #     print(\"❌ No FULL match\")\n\n    # if key4 in partial_match_dict:\n    #     print(\"🔁 Partial Match (4 fields)\")\n    #     print(\"📂 PARTIAL targets:\", dict(partial_match_dict[key4]))\n    # else:\n    #     print(\"❌ No PARTIAL match\")\n\n    # if key3 in item_only_dict:\n    #     print(\"🔁 Item-only Match (3 items)\")\n    #     print(\"📂 ITEM-ONLY targets:\", dict(item_only_dict[key3]))\n    # else:\n    #     print(\"❌ No ITEM-ONLY match\")\n\nrecall_at_3 = hits / len(sample_eval)\nprint(f\"\\n📊 Final Recall@3 on 5000-sample train set: {recall_at_3:.4f}\")\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:56:14.804351Z","iopub.execute_input":"2025-08-07T10:56:14.804641Z","iopub.status.idle":"2025-08-07T10:56:15.327373Z","shell.execute_reply.started":"2025-08-07T10:56:14.804618Z","shell.execute_reply":"2025-08-07T10:56:15.326343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.4 || Improvising Recommendation Algorithm","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nfrom collections import defaultdict, Counter\nfrom itertools import chain\nimport pandas as pd\nfrom tqdm import tqdm\n\n# ========= Step 1: Encode items =========\n\n# Get all unique items from sequences\nsequences = order_data[\"item_list\"].dropna().tolist()\nunique_items = set(chain.from_iterable(sequences))\n\n# Build item <-> id maps\nitem2id = {item: idx for idx, item in enumerate(sorted(unique_items))}\nid2item = {idx: item for item, idx in item2id.items()}\n\n# ========= Step 2: Encode train_df =========\n\ntrain_encoded = train_df.copy()\n\n# Encode and drop unknowns\ndef encode_or_none(item):\n    return item2id.get(item, None)\n\ntrain_encoded['input_item1'] = train_encoded['input_item1'].map(encode_or_none)\ntrain_encoded['input_item2'] = train_encoded['input_item2'].map(encode_or_none)\ntrain_encoded['input_item3'] = train_encoded['input_item3'].map(encode_or_none)\ntrain_encoded['target_item'] = train_encoded['target_item'].map(encode_or_none)\n\ntrain_encoded.dropna(inplace=True)\ntrain_encoded = train_encoded.astype({'input_item1': int, 'input_item2': int, 'input_item3': int, 'target_item': int})\n\n# ========= Step 3: Sort inputs =========\n\ntrain_encoded[['input_item1', 'input_item2', 'input_item3']] = (\n    train_encoded[['input_item1', 'input_item2', 'input_item3']]\n    .apply(lambda row: sorted(row), axis=1, result_type='expand')\n)\n\n# ========= Step 4: Build lookup dictionaries =========\n\ndef build_lookup(df):\n    full_match = defaultdict(Counter)\n    partial_match = defaultdict(Counter)\n    item_only = defaultdict(Counter)\n\n    for _, row in df.iterrows():\n        key5 = (row['STORE_NUMBER'], row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key4 = (row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key3 = tuple(sorted([row['input_item1'], row['input_item2'], row['input_item3']]))\n\n        full_match[key5][row['target_item']] += 1\n        partial_match[key4][row['target_item']] += 1\n        item_only[key3][row['target_item']] += 1\n\n    return full_match, partial_match, item_only\n\nfull_match_dict, partial_match_dict, item_only_dict = build_lookup(train_encoded)\n\n# ========= Step 5: Recommendation function =========\n\ndef get_recommendations(store_number, customer_type, item1, item2, item3):\n    items_sorted = sorted([item1, item2, item3])\n    key5 = (store_number, customer_type, *items_sorted)\n    key4 = (customer_type, *items_sorted)\n    key3 = tuple(items_sorted)\n\n    collected = []\n    seen = set()\n\n    for d in [full_match_dict, partial_match_dict, item_only_dict]:\n        if (key := key5 if d is full_match_dict else key4 if d is partial_match_dict else key3) in d:\n            for item, _ in d[key].most_common():\n                if item not in seen:\n                    collected.append(item)\n                    seen.add(item)\n                if len(collected) == 3:\n                    break\n        if len(collected) == 3:\n            break\n\n    while len(collected) < 3:\n        collected.append(-1)  # use -1 for \"NOT_FOUND\"\n\n    return collected\n\n# ========= Step 6: Evaluate Recall@3 =========\n\nsample_eval = train_encoded.sample(n=5000, random_state=42)\nhits = 0\n\nfor idx, row in tqdm(sample_eval.iterrows(), total=len(sample_eval)):\n    store = row['STORE_NUMBER']\n    cust_type = row['CUSTOMER_TYPE']\n    item1, item2, item3 = sorted([row['input_item1'], row['input_item2'], row['input_item3']])\n    target = row['target_item']\n\n    preds = get_recommendations(store, cust_type, item1, item2, item3)\n\n    if target in preds:\n        hits += 1\n\nrecall_at_3 = hits / len(sample_eval)\nprint(f\"\\n📊 Final Recall@3 on 2000-sample train set: {recall_at_3:.4f}\")\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:56:15.328328Z","iopub.execute_input":"2025-08-07T10:56:15.328657Z","iopub.status.idle":"2025-08-07T10:56:53.285493Z","shell.execute_reply.started":"2025-08-07T10:56:15.328629Z","shell.execute_reply":"2025-08-07T10:56:53.284536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🔄 Future Extensions\n\nWe can make this system even more powerful by:\n\n- Combining it with embedding-based models (hybrid recommendation).\n- Weighting recent transactions more heavily (recency bias).\n- Caching top-N results per key for real-time performance.","metadata":{}},{"cell_type":"markdown","source":"## 4.5 || Unsupervised Hybrid Addition","metadata":{}},{"cell_type":"code","source":"# Use entire dataset's item_list column (paid items only)\nsequences = order_data[\"item_list\"].dropna().tolist()\n\n# w2v_model = Word2Vec(sentences=sequences, vector_size=32, window=5, min_count=5, workers=4)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:56:53.286348Z","iopub.execute_input":"2025-08-07T10:56:53.286615Z","iopub.status.idle":"2025-08-07T10:56:53.427543Z","shell.execute_reply.started":"2025-08-07T10:56:53.286569Z","shell.execute_reply":"2025-08-07T10:56:53.426623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Vectorization","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nw2v_model = FastText(\n    sentences=sequences,\n    vector_size=256,     # same as Word2Vec\n    window=10,\n    min_count=3,\n    workers=4,\n    sg=1,                # skip-gram\n    negative=10,\n    epochs=30,\n    seed=42\n)\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:56:53.428557Z","iopub.execute_input":"2025-08-07T10:56:53.428913Z","iopub.status.idle":"2025-08-07T10:59:07.168738Z","shell.execute_reply.started":"2025-08-07T10:56:53.428884Z","shell.execute_reply":"2025-08-07T10:59:07.16775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Word to Vector converting function\ndef get_w2v(item):\n    try:\n        return w2v_model.wv[item].astype(np.float64)\n    except:\n        return np.zeros(32, dtype=np.float64)\n\n\n# Embed the 3 input items and concatenate\nitem_vectors = []\n\nfor _, row in train_df.iterrows():\n    v1 = get_w2v(row[\"input_item1\"])\n    v2 = get_w2v(row[\"input_item2\"])\n    v3 = get_w2v(row[\"input_item3\"])\n    vec = np.concatenate([v1, v2, v3])\n    item_vectors.append(vec)\n\nX = np.array(item_vectors, dtype=np.float64)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:59:07.169762Z","iopub.execute_input":"2025-08-07T10:59:07.170107Z","iopub.status.idle":"2025-08-07T10:59:39.602214Z","shell.execute_reply.started":"2025-08-07T10:59:07.170077Z","shell.execute_reply":"2025-08-07T10:59:39.601287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Unsupervised KMeans Algorithm","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nn_clusters = 256\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans.fit(X)\n\n# Assign cluster to each row\ntrain_df[\"cluster\"] = kmeans.labels_\n\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:59:39.60318Z","iopub.execute_input":"2025-08-07T10:59:39.603435Z","iopub.status.idle":"2025-08-07T11:23:13.153635Z","shell.execute_reply.started":"2025-08-07T10:59:39.603415Z","shell.execute_reply":"2025-08-07T11:23:13.152245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nfrom collections import defaultdict\n\ncluster_map = defaultdict(list)\n\nfor _, row in train_df.iterrows():\n    key = tuple(sorted([row[\"input_item1\"], row[\"input_item2\"], row[\"input_item3\"]]))\n    cluster_map[(row[\"cluster\"], key)].append(row[\"target_item\"])\n\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:23:13.155322Z","iopub.execute_input":"2025-08-07T11:23:13.155651Z","iopub.status.idle":"2025-08-07T11:23:31.989175Z","shell.execute_reply.started":"2025-08-07T11:23:13.155624Z","shell.execute_reply":"2025-08-07T11:23:31.988414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final Resort\n# Global item frequencies (for final backup)\nglobal_item_counts = Counter(train_encoded['target_item'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:23:31.990103Z","iopub.execute_input":"2025-08-07T11:23:31.990416Z","iopub.status.idle":"2025-08-07T11:23:32.042873Z","shell.execute_reply.started":"2025-08-07T11:23:31.99039Z","shell.execute_reply":"2025-08-07T11:23:32.041934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recommend_fourth_item(input_items, top_k=3):\n    input_items = sorted(input_items)\n    \n    vecs = []\n    for item in input_items:\n        try:\n            vec = get_w2v(item)\n            vec = np.array(vec, dtype=np.float64)  # Ensure float64\n            vecs.append(vec)\n        except KeyError:\n            continue  # Skip missing items\n    \n    # If we couldn't get all 3 vectors, return empty prediction\n    if len(vecs) != 3:\n        return []\n    \n    # Concatenate all 3 vectors into one flat vector\n    input_vec = np.concatenate(vecs).reshape(1, -1)  # shape: (1, vector_size * 3)\n    \n    # Predict cluster\n    cluster_id = kmeans.predict(input_vec)[0]\n    key = tuple(input_items)\n    \n    # Get candidates from the cluster map\n    candidates = cluster_map.get((cluster_id, key), [])\n    \n    # Rank by frequency\n    from collections import Counter\n    counts = Counter(candidates)\n    top_preds = [item for item, _ in counts.most_common(top_k)]\n    \n    return top_preds\n\n# def recommend_fourth_item(input_items, top_k=3):\n#     input_items = sorted(input_items)\n    \n#     vecs = []\n#     for item in input_items:\n#         try:\n#             vec = get_w2v(item)\n#             vec = np.array(vec, dtype=np.float64)\n#             vecs.append(vec)\n#         except KeyError:\n#             continue\n\n#     if len(vecs) != 3:\n#         return []\n\n#     input_vec = np.concatenate(vecs).reshape(1, -1)\n#     cluster_id = kmeans.predict(input_vec)[0]\n#     key = tuple(input_items)\n\n#     # Try exact triplet-based match\n#     candidates = cluster_map.get((cluster_id, key))\n    \n#     # Fallback to general cluster if needed\n#     if not candidates:\n#         candidates = cluster_map.get(cluster_id, [])\n\n#     if not candidates:\n#         return []\n\n#     counts = Counter(candidates)\n#     top_preds = [item for item, _ in counts.most_common(top_k)]\n    \n#     return top_preds\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:23:32.044061Z","iopub.execute_input":"2025-08-07T11:23:32.044422Z","iopub.status.idle":"2025-08-07T11:23:32.060733Z","shell.execute_reply.started":"2025-08-07T11:23:32.044394Z","shell.execute_reply":"2025-08-07T11:23:32.059894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nstart_time = time.time()\n# Sample 1000 rows from the train set\nsample_df = train_df.sample(n=2000, random_state=42)\n\nsuccess_count = 0\ntotal = 0\n\nfor _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n    input_items = sorted([row[\"input_item1\"], row[\"input_item2\"], row[\"input_item3\"]])\n    actual_target = row[\"target_item\"]\n\n    try:\n        predictions = recommend_fourth_item(input_items, top_k=3)\n        # print(predictions)\n        if actual_target in predictions:\n            success_count += 1\n        total += 1\n    except Exception as e:\n        print(f\"⚠️ Skipped row due to error: {e}\")\n        continue\n\n\nif total == 0:\n    print(\"❌ No valid rows were evaluated. Check recommend_fourth_item() logic.\")\nelse:\n    recall_at_3 = success_count / total\n    print(f\"✅ Recall@3 on {total} rows: {recall_at_3:.4f}\")\nprint(f\"\\n Time Taken: {time.time() - start_time}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:23:32.061551Z","iopub.execute_input":"2025-08-07T11:23:32.061875Z","iopub.status.idle":"2025-08-07T11:23:34.049119Z","shell.execute_reply.started":"2025-08-07T11:23:32.061846Z","shell.execute_reply":"2025-08-07T11:23:34.048239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.6 || Final Hybrid Model ","metadata":{}},{"cell_type":"markdown","source":"### Improvements in this version\n\n- No NOT_FOUND values to be encountered\n- Better Recommendations based on FastText vectorization and Clusters","metadata":{}},{"cell_type":"markdown","source":"### Complete Working Explanation\n1. Extracts unique items from `order_data` and creates `item2id` and `id2item` mappings.\n2. Encodes item columns in `train_df` using `item2id`, dropping unknown items.\n3. Sorts `input_item1`, `input_item2`, and `input_item3` in each row for consistency.\n4. Builds three lookup dictionaries:\n   * `full_match_dict`: Based on store, customer type, and 3 input items.\n   * `partial_match_dict`: Based on customer type and 3 input items.\n   * `item_only_dict`: Based on 3 input items only.\n     \n5. Defines `get_recommendations` function using a 5-level fallback strategy:\n   * Level 1: Exact match in `full_match_dict`.\n   * Level 2: Partial match in `partial_match_dict`.\n   * Level 3: Item-only match in `item_only_dict`.\n   * Level 4: Vector-based fallback using `recommend_fourth_item`.\n   * Level 5: Fill with globally frequent items.\n6. Samples 2000 rows from training data for evaluation.\n7. Computes Recall\\@3 by checking if target item is among top 3 predictions.\n8. Logs the number of failed predictions and total execution time.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\n# ========= Step 1: Encode items =========\n\n# Get all unique items from sequences\nsequences = order_data[\"item_list\"].dropna().tolist()\nunique_items = set(chain.from_iterable(sequences))\n\n# Build item <-> id maps\nitem2id = {item: idx for idx, item in enumerate(sorted(unique_items))}\nid2item = {idx: item for item, idx in item2id.items()}\n\n# ========= Step 2: Encode train_df =========\n\ntrain_encoded = train_df.copy()\n\n# Encode and drop unknowns\ndef encode_or_none(item):\n    return item2id.get(item, None)\n\ntrain_encoded['input_item1'] = train_encoded['input_item1'].map(encode_or_none)\ntrain_encoded['input_item2'] = train_encoded['input_item2'].map(encode_or_none)\ntrain_encoded['input_item3'] = train_encoded['input_item3'].map(encode_or_none)\ntrain_encoded['target_item'] = train_encoded['target_item'].map(encode_or_none)\n\ntrain_encoded.dropna(inplace=True)\ntrain_encoded = train_encoded.astype({'input_item1': int, 'input_item2': int, 'input_item3': int, 'target_item': int})\n\n# ========= Step 3: Sort inputs =========\n\ntrain_encoded[['input_item1', 'input_item2', 'input_item3']] = (\n    train_encoded[['input_item1', 'input_item2', 'input_item3']]\n    .apply(lambda row: sorted(row), axis=1, result_type='expand')\n)\n\n# ========= Step 4: Build lookup dictionaries =========\n\ndef build_lookup(df):\n    full_match = defaultdict(Counter)\n    partial_match = defaultdict(Counter)\n    item_only = defaultdict(Counter)\n\n    for _, row in df.iterrows():\n        key5 = (row['STORE_NUMBER'], row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key4 = (row['CUSTOMER_TYPE'],\n                row['input_item1'], row['input_item2'], row['input_item3'])\n\n        key3 = tuple(sorted([row['input_item1'], row['input_item2'], row['input_item3']]))\n\n        full_match[key5][row['target_item']] += 1\n        partial_match[key4][row['target_item']] += 1\n        item_only[key3][row['target_item']] += 1\n\n    return full_match, partial_match, item_only\n\nfull_match_dict, partial_match_dict, item_only_dict = build_lookup(train_encoded)\n\n# ========= Step 5: Recommendation function =========\ndef get_recommendations(store_number, customer_type, item1, item2, item3, use_fallback=True):\n    items_sorted = sorted([item1, item2, item3])\n    key5 = (store_number, customer_type, *items_sorted)\n    key4 = (customer_type, *items_sorted)\n    key3 = tuple(items_sorted)\n\n    collected = []\n    seen = set()\n\n    # 1️⃣ Try key5\n    for item, _ in sorted(full_match_dict.get(key5, {}).items(), key=lambda x: x[1], reverse=True):\n        if item not in seen:\n            collected.append(item)\n            seen.add(item)\n        if len(collected) == 3:\n            break\n\n    # 2️⃣ Try key4\n    if len(collected) < 3:\n        for item, _ in sorted(partial_match_dict.get(key4, {}).items(), key=lambda x: x[1], reverse=True):\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                break\n\n    # 3️⃣ Try key3\n    if len(collected) < 3:\n        for item, _ in sorted(item_only_dict.get(key3, {}).items(), key=lambda x: x[1], reverse=True):\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                break\n\n    # 4️⃣ Use KMeans cluster-based fallback if still short\n    if use_fallback and len(collected) < 3:\n        try:\n            num_needed = 3 - len(collected)\n            input_items = [id2item[i] for i in items_sorted if i in id2item]\n    \n            # Use a function like recommend_fourth_item(input_items, top_k)\n            fallback_items = recommend_fourth_item(input_items, top_k=num_needed)\n    \n            for f_item in fallback_items:\n                f_encoded = item2id.get(f_item)\n                if f_encoded is not None and f_encoded not in seen:\n                    collected.append(f_encoded)\n                    seen.add(f_encoded)\n                if len(collected) == 3:\n                    break\n        except Exception as e:\n            print(f\"⚠️ KMeans fallback failed: {e}\")\n\n    # 5️⃣ Final safety net: if still < 3, fill with most common items overall\n    if len(collected) < 3:\n        for item, _ in sorted(global_item_counts.items(), key=lambda x: x[1], reverse=True):\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                break\n\n    return collected\n\n\n    # 5️⃣ Final safety net: if still < 3, fill with most common items overall\n    if len(collected) < 3:\n        for item, _ in global_item_counts.most_common():\n            if item not in seen:\n                collected.append(item)\n                seen.add(item)\n            if len(collected) == 3:\n                break\n\n    return collected\n\n# ========= Step 6: Evaluate Recall@3 =========\nsample_eval = train_encoded.sample(n=5000, random_state=42)\nhits = 0\nnot_found_count = 0\n\nfor idx, row in tqdm(sample_eval.iterrows(), total=len(sample_eval)):\n    store = row['STORE_NUMBER']\n    cust_type = row['CUSTOMER_TYPE']\n    item1, item2, item3 = sorted([row['input_item1'], row['input_item2'], row['input_item3']])\n    target = row['target_item']\n\n    preds = get_recommendations(store, cust_type, item1, item2, item3)\n\n    if -1 in preds:\n        not_found_count += 1\n    if target in preds:\n        hits += 1\n\nrecall_at_3 = hits / len(sample_eval)\nprint(f\"\\n=====> Final Recall@3 on 1000-sample train set: {recall_at_3:.4f}\")\nprint(f\"=====> Number of NOT_FOUND predictions: {not_found_count}\")\n\n\nprint(\"Time taken: \", time.time() - start_time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:33:04.783889Z","iopub.execute_input":"2025-08-07T11:33:04.78431Z","iopub.status.idle":"2025-08-07T11:33:42.661031Z","shell.execute_reply.started":"2025-08-07T11:33:04.784279Z","shell.execute_reply":"2025-08-07T11:33:42.659937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.7 || Cluster Visualization","metadata":{}},{"cell_type":"code","source":"# Convert X_vectors (list of np.arrays) into a 2D array\nX_array = np.vstack(X)\n\n# Get cluster labels\ncluster_ids = kmeans.labels_\n\n# Reduce to 2D for plotting\npca = PCA(n_components=2)\nX_2d = pca.fit_transform(X_array)\n\n# Plot the clusters\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_ids, palette='tab10', s=40)\nplt.title(\"Clusters of Item Triplets (PCA Projection)\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:24:11.430889Z","iopub.execute_input":"2025-08-07T11:24:11.431147Z","iopub.status.idle":"2025-08-07T11:24:35.486611Z","shell.execute_reply.started":"2025-08-07T11:24:11.431127Z","shell.execute_reply":"2025-08-07T11:24:35.485487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 || Final Tests and Submission","metadata":{}},{"cell_type":"markdown","source":"## 5.1 || Application Testing Simulation","metadata":{}},{"cell_type":"code","source":"def test_recommender_interactive(\n    default_store=2156,\n    default_cust_type=\"Registered\",\n    default_item1=\"6 pc Grilled Wings Combo\",\n    default_item2=\"20 pc Grilled Wings\",\n    default_item3=\"20pc Spicy Feast Deal\"\n):\n    print(\"<===== Wings R Us Recommender Test =====>\")\n\n    try:\n        # # Prompt with defaults\n        # store_input = input(f\"Enter STORE_NUMBER [{default_store}]: \").strip()\n        # cust_type_input = input(f\"Enter CUSTOMER_TYPE [{default_cust_type}]: \").strip()\n        # item1_input = input(f\"Enter item1 name [{default_item1}]: \").strip()\n        # item2_input = input(f\"Enter item2 name [{default_item2}]: \").strip()\n        # item3_input = input(f\"Enter item3 name [{default_item3}]: \").strip()\n        \n        # store = int(store_input) if store_input else default_store\n        # cust_type = cust_type_input if cust_type_input else default_cust_type\n        # item1 = item1_input if item1_input else default_item1\n        # item2 = item2_input if item2_input else default_item2\n        # item3 = item3_input if item3_input else default_item3\n        store = default_store\n        cust_type = default_cust_type\n        item1 = default_item1\n        item2 = default_item2\n        item3 = default_item3\n\n        # Encode items\n        item_inputs = []\n        for item in [item1, item2, item3]:\n            if item not in item2id:\n                print(f\"⚠️ Item '{item}' not found in item2id. Recommendation may be less accurate.\")\n            item_inputs.append(item2id.get(item, None))\n\n        if None in item_inputs:\n            print(\"❌ One or more items not recognized. Please try with known items.\")\n            return\n\n        # Get recommendations\n        encoded_recs = get_recommendations(store, cust_type, *item_inputs)\n        decoded_recs = [id2item.get(i, \"UNKNOWN\") for i in encoded_recs]\n\n        # Output\n        print(\"\\n✅ Recommended items:\")\n        for idx, item in enumerate(decoded_recs, 1):\n            print(f\"  {idx}. {item}\")\n\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        \ntest_recommender_interactive()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:34:08.043707Z","iopub.execute_input":"2025-08-07T11:34:08.044437Z","iopub.status.idle":"2025-08-07T11:34:08.054116Z","shell.execute_reply.started":"2025-08-07T11:34:08.044403Z","shell.execute_reply":"2025-08-07T11:34:08.053172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2 || Creating Submission File","metadata":{}},{"cell_type":"code","source":"submission_rows = []\n\n# Build submission rows from test_df\nfor idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n    store = row['STORE_NUMBER']\n    cust_type = row['CUSTOMER_TYPE']\n    \n    input_items = [row['item1'], row['item2'], row['item3']]\n    encoded_items = [item2id.get(item) for item in input_items]\n\n    if None in encoded_items:\n        continue  # Skip unknown items\n\n    sorted_encoded = sorted(encoded_items)\n    recs_encoded = get_recommendations(store, cust_type, *sorted_encoded)\n    recs_decoded = [id2item.get(i, \"UNKNOWN\") for i in recs_encoded]\n\n    while len(recs_decoded) < 3:\n        recs_decoded.append(\"\")\n\n    submission_rows.append({\n        \"CUSTOMER_ID\": row[\"CUSTOMER_ID\"],\n        \"ORDER_ID\": row[\"ORDER_ID\"],\n        \"item1\": input_items[0],\n        \"item2\": input_items[1],\n        \"item3\": input_items[2],\n        \"item4\": \"Missing\",\n        \"RECOMMENDATION 1\": recs_decoded[0],\n        \"RECOMMENDATION 2\": recs_decoded[1],\n        \"RECOMMENDATION 3\": recs_decoded[2]\n    })\n\n# Convert to DataFrame\nsubmission_df = pd.DataFrame(submission_rows)\n\n# Save to Excel Sheet (As per instructions)\nteam_name = \"Gods\"  \noutput_filename = f\"{team_name}_Recommendation_Output_Sheet.csv\"\nsubmission_df.to_csv(f\"{output_filename}\", index=False)\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:34:54.628408Z","iopub.execute_input":"2025-08-07T11:34:54.628814Z","iopub.status.idle":"2025-08-07T11:34:54.859533Z","shell.execute_reply.started":"2025-08-07T11:34:54.628773Z","shell.execute_reply":"2025-08-07T11:34:54.858732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.3 || Test for Invalid Entries","metadata":{}},{"cell_type":"code","source":"invalid_values = {\"\", None, \"NOT_FOUND\", \"UNKNOWN\", -1}\n\nfor col in submission_df.columns:\n    if \"RECOMMENDATION\" in col:\n        print(f\"Checking column: {col}\")\n        \n        # Show value counts\n        counts = submission_df[col].value_counts(dropna=False)\n        display(counts.sort_index())\n\n        # Check for invalid entries\n        invalid_entries = submission_df[col].isin(invalid_values)\n        num_invalid = invalid_entries.sum()\n\n        if num_invalid > 0:\n            print(f\"⚠️ Found {num_invalid} invalid entries in {col}\")\n        else:\n            print(\"✅ All entries are valid.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:39:29.595346Z","iopub.execute_input":"2025-08-07T11:39:29.595765Z","iopub.status.idle":"2025-08-07T11:39:29.618145Z","shell.execute_reply.started":"2025-08-07T11:39:29.595734Z","shell.execute_reply":"2025-08-07T11:39:29.617313Z"}},"outputs":[],"execution_count":null}]}